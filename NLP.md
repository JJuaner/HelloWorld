文本处理（文本信息：
词法分析（分词/形态变化，词性标注/实体识别）
句法分析（结构，依存）
语义分析（实体关系，逻辑）
图5.9


zipf law
数据稀疏问题-0概率-数据平滑

对数线性模型
log，变成+

判断两个变量的独立性


### 文本表示
机器不认识文本，认识数字，表示为数字：文本表示
好的文本表示应该反映更多的信息

词袋模型：ID->onehot(高维，仅仅做区别没有反映语义相似性都正交/没有任何语义信息)->分布表示（SLI，word embedding：word2vec：CBOW/skip-gram
高维->特征工程/特征选择（最相关最有分别力的特征）：freq，stopword，mutual，x2
CBOW：windowsize 输入输出index-onehot 输入生成输出监督学习输入W和输出W‘/VN  （Wt，W't/NV包含词压缩表示，onehot线性组合表示乘处理，

句子的表示：
1.词向量->句子向量   
2.扩展word2vec（2）直接生成

文本分类：
`朴素贝叶斯方法`  
`P(ck|D)=P(D|ck)P(ck)`
D的表示->计算的项->训练：项的数值来自训练数据的估计 估计测试：公式，代入
D的表示两种算法：D(扩展onehot)-项(01出现不出现,频度)-->不同计算

训练：表示文档D(V+2)，统计信息NNk->参数估计P(wt|ck) P(ck)->P(D|ck) P(c|k)->贝叶斯
P(c|k)=Nk/N
ck内所有文档：有的文档/总文档数
ck内所有文档：词wt的频度之和/所有词频度之和
构造P(D|ck)
贝叶斯

零概率问题-构造等价类/平滑

词向量->句子向量  
TF-IDF文档频率和逆文档频率  
--两种思想：一个词，1.文档内出现频率越高越重要，2.其他文档内也出现越不重要  
--计算tf=fij/max{fij} idf=log(N/n)  



文本表示:词袋模型，词向量句子向量
构建分类模型
评估结果



RNN形式
问题：网络层数加深，梯度消失梯度爆炸
模型学习：抓住单元unit（输入，输出，内部）
RNN
LSTM
GRU




## 机器翻译
发展历程(资源越来越多，计算能力越来越强)
–基于规则的机器翻译 
–基于实例的机器翻译 
–统计机器翻译
–神经⺴络机器翻译

基于规则的机器翻译：需要语⾔学家⼤量的⼯作维护难度⼤；翻译规则容易发⽣冲突

机器翻译容易受到输⼊噪⾳的影响
•⼈⼯制造噪⾳，加强模型抗干扰能⼒
