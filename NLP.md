文本处理（文本信息：
词法分析（分词/形态变化，词性标注/实体识别）
句法分析（结构，依存）
语义分析（实体关系，逻辑）
图5.9


从数据中得到这个表
计算方法是确定的
参数估计
## 文本分类
P(ck|D)=朴素贝叶斯转换P(D|ck)P(ck)
两个参数的计算
>>1.贝努力文档模型：文档=词表向量+出现不出现/文档被表示为词表的二值特征向量  依词表计算！！！P(D|ck)=考虑词的出现不出现
>>2.多项式文档模型：文档=词表向量+出现次数/文档被表示为词表的计数特征向量，不仅词是否出现，还记录了出现次数  依出现词计算！！！P(D|ck)=考虑出现词项的出现次数，不考虑词的不出现/C=

文档的词袋模型：词的表示 P(D|ck)的计算 训练语料-模型参数估计词在类别中的概率P(w|ck)
都不考虑词的位置和次序

0概率问题



计算

文本分类效果评价
二分类：准确率，精准率，召回率，F1
多分类：micro所有类别一起按照定义计算 macro每个类别单独计算取平均



## 情感分析
文档级
句子级
细粒度基于方面的情感分类
观点挖掘
（其他任务：观点总结，情感原因提取，假新闻谣言识别）
（应用场景：社交媒体新闻舆情，电影等产品评论）

情感分类中的特殊问题：否定/转折/副词增强
观点是个三元组/四元组（目标对象，对目标的情感，观点持有者，观点时间）（实体，方面，情感，持有者，时间）


## 词性标注/HMM/HMM、用于词性标注
词性标注问题：兼类词歧义
决定词性：语言学角度：词的用法和句中语法功能/统计学角度：上下文的词和上下文的词性
常见方法：
1.规则方法（词典提供候选词性，人工整理标注规则）
2.基于错误驱动的方法
3.统计方法：问题的形式化描述/建立统计模型（HMM，最大熵，条件随机场，结构化支持向量机）

词性标注性能基线：90%
词性标注to句法分析/机器翻译


生成模型，从头
前向后向 使用动态规划加速计算
前向：当前观察约束下，从头1时刻所有路径到i状态的概率  （计算=（其他所有状态量（前驱）×转移概率）求和×其生成观测量的概率
后向：当前观察约束下，从最后时刻T所有路径到j状态的概率 （计算=（其他所有状态量（后继）×其生成观测量的概率×转移概率）求和

## 语言模型
语言模型：计算一句话的概率/词串序列存在的可能性
建模P(W)（模型参数估计/计算）：就可以计算P(W)
n-gram模型
线性语言模型
神经网络语言模型


n-gram模型=n-1阶马尔科夫链（有限视野假设：当前词的出现概率只和前n-1个词有关）：P(W)=
训练语料参数估计（参数量）（最大似然估计/选择的参数对于训练数据给出了最大概率，样本空间越大，估计值越真实：相对频率）
计算
zipf law：词频和rank的乘积是常数：语言中频繁出现的事件是有限的，大部分词都稀有/不可能搜集到足够的数据来得到稀有事件的完整概率分布。词（一元）如此，对于二元、三元模型更加严重
数据稀疏问题/0概率（没有足够的训练数据，对于未观测到的数据，出现零概率现象）-构造等价类/数据平滑（加一平滑，拉普拉斯平滑，线性插值）

神经网络语言模型（FNNLM，RLM，biRLM，MLM
线性语言模型

（最简单的统计语言模型，简单有效/数据稀疏问题泛化能力，长依赖没有考虑语法句法结构信息）
ngram VS cache
ngram VS FNNLM（简单有效/复杂最好，数据稀疏问题泛化能力差/特征提取泛化能力）
模型评价：外在评价（NLP任务效果）内在评价（困惑度：测试集数据存在的概率乘积-log求和-2-l/越小越好）

## 文本表示
机器和算法不认识文本，文本需要表示为数字：（词和文档）文本表示
好的文本表示应该能反映更多的语法结构信息
文本表示评价：外在评价（nlp任务效果）内在评价（语义相似度）

词的基本表示：ID编码onehot向量（高维，正交）/标准符号编码中，每个词是一个维度/仅仅做区别没有反映语义相似性/没有任何语义信息)->分布表示（SLI，word embedding：word2vec：CBOW/skip-gram
句子文档的基本表示：词袋模型（1.onehot词袋：词onehot向量累加/往预定义词表里填句子词，2.n-gram词袋/以n个词组织词表）
*（组织表示！！！！预定义词表为对象！！而不是句子为对象！！句子往词表里填：稀疏0概率问题，没有考虑位置和顺序关系）*

高维-->特征工程/特征选择（最重要最相关最有分别力的特征/目标是表示词表示文档，这里或者说是选词）：stopword，freq，mutual互信息，x2卡方检测

分布式基于相似度的表示：
潜在语义分析/SCI：词-文档的共现矩阵X:n×m 
问题：非常大/更低维的表示 
方法：奇异值分解然后近似（SVD计算复杂度，K的选择，仍是线性模型/新词重新算，没考虑位置词序）
与传统向量空间模型的比较：也用向量表示词和文档并用夹角表示相似度，不同在于映射到潜在语义空间，避免了原空间噪音

word embedding/word2vec
1.CBOW：基于上下文预测当前词，windowsize 输入输出index-onehot 输入生成输出监督学习输入W和输出W‘/VN  （Wt，W't/NV包含词压缩表示，onehot线性组合表示乘处理，
2.skipgram：基于当前词预测上下文

句子文档的表示：
1.词向量->句子向量   
2.扩展word2vec模型直接生成向量（2个模型）

文本分类：
`朴素贝叶斯方法`  
`P(ck|D)=P(D|ck)P(ck)`
D的表示->计算的项->训练：项的数值来自训练数据的估计 估计测试：公式，代入
D的表示两种算法：D(扩展onehot)-项(01出现不出现,频度)-->不同计算

训练：表示文档D(V+2)，统计信息NNk->参数估计P(wt|ck) P(ck)->P(D|ck) P(c|k)->贝叶斯
P(c|k)=Nk/N
ck内所有文档：有的文档/总文档数
ck内所有文档：词wt的频度之和/所有词频度之和
构造P(D|ck)
贝叶斯

零概率问题-构造等价类/平滑


TF-IDF文档频率和逆文档频率  
--两种思想：一个词，1.（局部信息）文档内出现频率越高越重要，2.（全局信息）其他文档内也出现越不重要  
--计算tf=fij/max{fij} idf=log(N/n)  
--用途：计算这个词对句子/文档的重要性：1.特征选择 2.词向量->句子向量  


文本表示:词袋模型，词向量句子向量
构建分类模型
评估结果


朴素贝叶斯
一个到一个
比如文本分类，机器翻译

RNN循环神经网络形式
问题：BP链式法则训练问题，网络层数加深梯度消失梯度爆炸
RNN/LSTM/GRU模型记忆/学习：抓住单元unit（输入，输出，内部）

## 句法分析：组成成分分析/结构分析（词如果构成短语，短语如何构成句子）和依存分析（以动词为中心）
形式文法G是一个四元组{NTSP}
形式语言：由形式文法生成的语言，由终结符组成的串，句型是带有非终结符的中间结果

上下文无关文法（2型文法）
（作用：1.可以用来描述大部分自然语言 2.基于CFG的句法分析器：G+S-句法分析树/难以避免歧义）
传统CFG在描述自然语言时存在的问题：不合法的句子
解决方法：1.增加句法符号和规则（增加数量和潜在冗余/语言结构描述缺乏深度） 2.基于特征的扩展CFG（不增加原CFG中的句法符号，给每个句法符号增加特征属性(名值对)，
PCFG问题：独立性假设过强（比如np是依赖于父亲np）
解决方法：1.修改文法 2.词汇化的PCFG

文法的乔姆斯基范式：N->NN N->T 多叉树变成二叉树，分析更简单


句法分析过程
文法G/CFG 
待分析句子
XXXXXXXXXXXXXXXXXX
得到推导序列！语法分析树！！



分析方法/基于CFG的分析器：
自顶向下：从S推导出句子（最左推导S->wwwwww
自底向上：从句子归约为S
回溯：推导和归约有多个产生式供选择，当用其中一个发生错误时返回尝试另一个（分支



CFG,句子->句法分析树/句子的分析过程
改写句子位置计数
当前状态，后备状态 使用规则
((S)1)
(每步：取当前状态符号表的第一个符号，找是否有产生式，1.如果为句法符号：新状态 当前状态/后备状态 2.如果为词法符号：匹配字串，计数加1，不能匹配则失败）
如果当前状态符号表为空，1.计数到达末尾：成功  2.计数在句子中间：回溯（取一个后备状态继续分析）
无状态了，失败
*后备状态的存储：栈/队列（后备状态少，存储效率高，但存在左递归问题）*

CFG,句子->句法分析树/句子的分析过程
句子本身* R/S
每一步总是移进or归约！
CFG,句子->句法分析树/句子的分析过程
基于图的分析法
改写句子位置计数图
取agenda（若agenda为空，则把句子中下一个词的各种词法符号（词性）和它们的位置加入进来） 查看是否形成活动边 chart （扩展活动边：chart是否接续，完全匹配加入agenda或者继续扩大活动边）
理解活动边(未完全匹配)，agenda/chart（已匹配）


PCFG：每个产生式带概率
计算特定树的概率
计算句子生成/存在概率
解码：句子对应概率最大的树
句法分析：自底向上归约，带概率的CYK算法

treebank
构建树库，从树库中抽取文法规则
概率估计：最大似然估计 有监督(树库):计数-相对频率  无监督(没树库)：EM算法，利用向内概率和向外概率
计算特定树的概率，确定的
句子概率和解码问题


句子概率（句子生成/存在概率）
全枚举思路:/P(W1n,t|G)求和
动态规划思路：向内算法向外算法（αin(S)）（遍历k遍历非终结符 产生wP×βkk(A)求和）

解码


base，理解单独定义，本身求得（如何计算/计算过程），应用时直接使用？？
理解意义!!!变量/符号意义
向内概率αij(A) 给定非终结符号A，生成子串Wij的概率(计算：遍历所有产生式和w切分k)
向外概率βij(A) 从开始符号开始，生成非终点符号A以及之外的其它子串的概率(计算：)



无论是自顶向下还是自底向上问题：自然语言难以避免总是有歧义（每一步多个产生式可选
句法分析器性能评价：精准率/召回率


会算会描述
## 信息抽取IE
从非结构化，半结构化的数据中提取结构化信息。形成知识
任务：实体识别，关系抽取，事件抽取（who did what to whom when），知识库填充（实体链接和消歧义）
实体识别：1.规则方法 2.统计方法
关系抽取：1.封闭域：有限的实体类型和关系类型（有监督的统计学习方法） 2.开放域：实体关系类型众多（语言的多样性和有限性，样本的不平衡和稀疏）（基于模板的方法）
传统的关系分类：依赖大量标记数据
基于远程监督的关系分类：自动产生大量标记数据，假设（问题：假设太强（假阳性，假阴性）->数据噪音，解决：抑制/移除/修正噪音，额外的知识取增强模型）
## 机器翻译
发展历程(资源越来越多，计算能力越来越强)
–基于规则的机器翻译 
–基于实例的机器翻译 
–统计机器翻译
–神经⺴络机器翻译

基于规则的机器翻译：需要语⾔学家⼤量的⼯作维护难度⼤；翻译规则容易发⽣冲突
基于实例的机器翻译：查找相似翻译，类比替换
统计机器翻译：从双语平行语料中自动获取翻译规则（词共现关系-词对齐，对应关系图(画矩形理解！上下左右没有)-短语规则(问题是多种组合情况<=>短语嵌套)，调序（顺序翻译，基于距离发限制调序，词汇化的调序模型，利用语言模型建模句子流畅度））
（能一定程度上能自动从数据中挖掘翻译规则，流程复杂，性能有限）（难度在于翻译和语言的多样性，意译，省略）
神经网络机器翻译：直接把句子看成单词序列，序列到序列的翻译方式，利用注意力机制动态获取信息（Bi-directional RNN + Attention）
（从规则建模和统计稀疏性两个方面提升了机器翻译系统，更充分发挥机器能力和长处的）

机器翻译的评估：



机器翻译研究热点：
机器翻译容易受到输⼊噪⾳的影响
•⼈⼯制造噪⾳，加强模型抗干扰能⼒

机器翻译的评价
1.人工评价
2.自动评价
评价标准：人工翻译结果作为参考译文，使用多个增加鲁棒性
比较句子相似性：
WER（考虑编辑距离，对顺序敏感），PER（不考虑顺序，只考虑单词的匹配），
BLEU=Unigram Precision的修改+•Brevity Penalty
Un/nigram Precision在参考译文里出现过计数/总数（thethethe例子，一个词有计数上限）（短句子-recall，but也不对）（
TER（避免译文之间相互干扰：取一个译文最匹配/edits编辑最少的译文 =edits/译文的平均长度）
HTER：人工编辑翻译结果直到正确，获得距离最近的参考译文
基于图（构造参考译文图）






注意力机制qkv 
q和k计算相似度分数（点积，general。cancat）-通过softmax后得到权重-v权重求和
Q=K=V for self-attention




--------


HMM 生成模型/模型参数：三个表格
三个基本问题
1.给定模型参数，计算观测序列的概率/这个模型/参数生成观测序列的概率、   给定模型，生成模型，观测序列的生成概率/观测序列的存在概率
2.给定模型参数和观测序列，观测序列的最优最有可能的隐状态序列          给定模型和观测序列，解码
3.给定观测序列集合，估计模型参数

问题一：计算观测序列的概率
各种隐状态生成观测状态的概率之和
图-路径之和，路径是边的乘积
思路1：枚举各种隐状态序列再生成观测序列，概率之和（搜索空间(图)太大（其实模型生成隐状态序列的概率是固定的，只受前一个隐状态影响不受观测状态监督）
思路2：动态规划
前向概率：前t个观测状态序列且第t个隐状态为i的联合概率
⼀个节点的前向概率是从t=1时刻所有节点出发到达该节点的所有路径之和/所有路径到底节点的概率之和
后向概率：第t个隐状态为j情况下部分观测状态序列 与的条件概率

问题二：给定模型参数和观测序列，观测序列的最优最有可能的隐状态序列
思路一：枚举各个隐状态序列，计算转换概率和生成概率，取最大
思路二：动态规划
维特比算法：每个节点记录的不再是路径之和而是记录路径中最大值，额外记录最大前继


假设过强
ngram/马尔可夫
PCFG
基于远程监督的关系分类


存在概率
前向算法/向内算法：使用α（α计算推导2+1
后向算法/向外算法：使用β（β计算推导2+1
解码
前向算法，向内算法的修改


性能评价
语言模型：效果/困惑度
文本表示：效果/相似度
文本分类：二分类，多分类
词性标注：准确率
句法分析：精准/召回
机器翻译：人工/自动

情感分析
信息抽取
