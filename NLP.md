文本处理（文本信息：
词法分析（分词/形态变化，词性标注/实体识别）
句法分析（结构，依存）
语义分析（实体关系，逻辑）
图5.9



对数线性模型
log，变成+

判断两个变量的独立性

从数据中得到这个表
计算方法是确定的
参数估计

## 情感分析
文档级
句子级
细粒度基于方面的情感分类
观点挖掘
（其他任务：观点总结，情感原因提取，假新闻谣言识别）
（应用场景：社交媒体新闻舆情，电影等产品评论）

情感分类中的特殊问题：否定/转折/副词增强
观点是个三元组/四元组（目标对象，对目标的情感，观点持有者，观点时间）（实体，方面，情感，持有者，时间）


## 词性标注/HMM/HMM、用于词性标注
词性标注问题：兼类词歧义
决定词性：语言学角度：词的用法和句中语法功能/统计学角度：上下文的词和上下文的词性
常见方法：
1.规则方法（词典提供候选词性，人工整理标注规则）
2.基于错误驱动的方法
3.统计方法：问题的形式化描述/建立统计模型（HMM，最大熵，条件随机场，结构化支持向量机）

词性标注性能基线：90%
词性标注to句法分析/机器翻译


生成模型，从头
前向后向 使用动态规划加速计算
前向：当前观察约束下，从头1时刻所有路径到i状态的概率  （计算=（其他所有状态量（前驱）×转移概率）求和×其生成观测量的概率
后向：当前观察约束下，从最后时刻T所有路径到j状态的概率 （计算=（其他所有状态量（后继）×其生成观测量的概率×转移概率）求和

## 语言模型
语言模型：计算一句话的概率/词串序列存在的可能性
建模P(W)（模型参数估计/计算）：就可以计算P(W)
n-gram模型
线性语言模型
神经网络语言模型


n-gram模型=n-1阶马尔科夫链（有限视野假设：当前词的出现概率只和前n-1个词有关）：P(W)=
训练语料参数估计（参数量）（最大似然估计/选择的参数对于训练数据给出了最大概率，样本空间越大，估计值越真实：相对频率）
计算
zipf law：词频和rank的乘积是常数：语言中频繁出现的事件是有限的，大部分词都稀有/不可能搜集到足够的数据来得到稀有事件的完整概率分布。词（一元）如此，对于二元、三元模型更加严重
数据稀疏问题/0概率（没有足够的训练数据，对于未观测到的数据，出现零概率现象）-构造等价类/数据平滑（加一平滑，拉普拉斯平滑，线性插值）

神经网络语言模型（FNNLM，RLM，biRLM，MLM
线性语言模型

（最简单的统计语言模型，简单有效/数据稀疏问题泛化能力，长依赖没有考虑语法句法结构信息）
ngram VS cache
ngram VS FNNLM（简单有效/复杂最好，数据稀疏问题泛化能力差/特征提取泛化能力）
模型评价：外在评价（NLP任务效果）内在评价（困惑度：测试集数据存在的概率乘积-log求和-2-l/越小越好）

## 文本表示
机器和算法不认识文本，文本需要表示为数字：（词和文档）文本表示
好的文本表示应该能反映更多的语法结构信息
文本表示评价：外在评价（nlp任务效果）内在评价（语义相似度）

词的基本表示：ID编码onehot向量（高维，正交）/标准符号编码中，每个词是一个维度/仅仅做区别没有反映语义相似性/没有任何语义信息)->分布表示（SLI，word embedding：word2vec：CBOW/skip-gram
句子文档的基本表示：词袋模型（1.onehot词袋：词onehot向量累加/往预定义词表里填句子词，2.n-gram词袋/以n个词组织词表）
*（组织表示！！！！预定义词表为对象！！而不是句子为对象！！句子往词表里填：稀疏0概率问题，没有考虑位置和顺序关系）*

高维-->特征工程/特征选择（最重要最相关最有分别力的特征/目标是表示词表示文档，这里或者说是选词）：stopword，freq，mutual互信息，x2卡方检测

分布式基于相似度的表示：
潜在语义分析/SCI：词-文档的共现矩阵X:n×m 
问题：非常大/更低维的表示 
方法：奇异值分解然后近似（SVD计算复杂度，K的选择，仍是线性模型/新词重新算，没考虑位置词序）
与传统向量空间模型的比较：也用向量表示词和文档并用夹角表示相似度，不同在于映射到潜在语义空间，避免了原空间噪音

word embedding/word2vec
1.CBOW：基于上下文预测当前词，windowsize 输入输出index-onehot 输入生成输出监督学习输入W和输出W‘/VN  （Wt，W't/NV包含词压缩表示，onehot线性组合表示乘处理，
2.skipgram：基于当前词预测上下文

句子文档的表示：
1.词向量->句子向量   
2.扩展word2vec模型直接生成向量（2个模型）

文本分类：
`朴素贝叶斯方法`  
`P(ck|D)=P(D|ck)P(ck)`
D的表示->计算的项->训练：项的数值来自训练数据的估计 估计测试：公式，代入
D的表示两种算法：D(扩展onehot)-项(01出现不出现,频度)-->不同计算

训练：表示文档D(V+2)，统计信息NNk->参数估计P(wt|ck) P(ck)->P(D|ck) P(c|k)->贝叶斯
P(c|k)=Nk/N
ck内所有文档：有的文档/总文档数
ck内所有文档：词wt的频度之和/所有词频度之和
构造P(D|ck)
贝叶斯

零概率问题-构造等价类/平滑

词向量->句子向量  
TF-IDF文档频率和逆文档频率  
--两种思想：一个词，1.文档内出现频率越高越重要，2.其他文档内也出现越不重要  
--计算tf=fij/max{fij} idf=log(N/n)  



文本表示:词袋模型，词向量句子向量
构建分类模型
评估结果


朴素贝叶斯
一个到一个
比如文本分类，机器翻译

RNN循环神经网络形式
问题：BP链式法则训练问题，网络层数加深梯度消失梯度爆炸
RNN/LSTM/GRU模型记忆/学习：抓住单元unit（输入，输出，内部）





## 机器翻译
发展历程(资源越来越多，计算能力越来越强)
–基于规则的机器翻译 
–基于实例的机器翻译 
–统计机器翻译
–神经⺴络机器翻译

基于规则的机器翻译：需要语⾔学家⼤量的⼯作维护难度⼤；翻译规则容易发⽣冲突

机器翻译容易受到输⼊噪⾳的影响
•⼈⼯制造噪⾳，加强模型抗干扰能⼒
  
