## 5 聚类
## 6 降维
KNN：利用k个最近邻的信息来做预测（要点：k值，距离度量，如何利用:投票平均加权）
KNN特殊地位：1.懒惰算法，无需训练，但测试时需需要遍历一遍训练集 2.简单有效，泛化错误率不超过贝叶斯最优分类器错误率的两倍
懒惰算法、理论性质、现实不可行：KNN需要密采样，维度->样本数，现实情况样本少，样本稀疏
高维空间的维度灾难：计算开销大，最重要的是样本稀疏->降维，数据维度大，但任务相关的可能只是部分维度，高维空间的低维嵌入
## 7 特征选择和稀疏表示
特征：定义，分类
特征选择：why
特征选择的一般方法：子集搜索，子集评估
特征选择方法：1.过滤式（relief，相关统计量）2.包裹式（lvw）3.嵌入式（L1正则化，w稀疏解非0分量，求解）

稀疏表示
将数据集看作一个矩阵，每行对应一个样本，每列对应一个特征
稀疏性：这个矩阵有很多0元素，但不是整行整列出现
稀疏表示的好处：1.高效存储 2.问题线性可分（svm文本数据）

学习一个字典，样本稠密表示->恰当的稀疏表示：字典学习
字典学习的目标是学校字典矩阵B，优化目标形式（1项最小化重构误差，2.尽量稀疏），求解（比L1更难解，变量交替优化（逐列更新策略））

压缩感知：限定等距性 优化形式（转换为共解的最小化L1范式问题）
矩阵补全：稀疏矩阵 优化形式（转换为矩阵核范数=奇异值之和->半正定规划）
## 8 半监督学习
主动学习、半监督学习、直推学习
未标记样本的假设（本质上都是：相似的样本具有相似的输出）
### 生成式方法
假设所有样本由一个潜在的生成式模型生成，EM算法极大似然估计求解
假设样本独立同分布，由一个高斯混合模型生成，每个类别对应一个高斯混合成分
这类方法的区别在于对生成式模型的假设，不同的假设形成不同的生成式方法
关键：模型假设必须正确，即假设的生成式模型必须与数据的真实分布吻合
适用于标签极少的情况
### 半监督SVM-TSVM
二分类问题
TSVM 思想（考虑为每个未标记样本分别为正例和负例的情况，然后在这些情况中找到间隔最大的划分超平面，这个超平面形成的划分作为最终的预测结果）和优化形式 
穷举过程->（未标记数量少，更高效）->TSVM局部搜索策略迭代的寻找近似解
算法描述，类别不平衡问题的改进，问题和改进

### 图半监督
给定数据集，将其映射为一个图（每个节点是一个样本，如果两个样本比较相似，则存在一条边，边的强度正比于样本的相似度）
图对应一个矩阵，基于矩阵运算来推导算法
图半监督学习的思想：颜色在图上的传播过程

### 基于分歧的方法
多学习器，学习器间的分歧
多视图数据-协同训练（充分且条件独立，相容互补性--显著提升模型效果，更弱的情况也可以）
单视图数据-协同训练的变体（无需多视图，构建学习器之间的差异和分歧就可提升模型泛化性能）

### 拓展
半监督深度学习
动机
早期半监督深度学习方法（虽然用了有/无标签数据，单还是有监督的训练）：1.无监督预训练加微调（利用无标签数据让网络有好的初始化） 2.作为特征提取器
端到端的半监督深度学习两个基本方法：1.熵最小化（鼓励模型输出的预测置信度尽可能高，对未标记样本生成熵最小化的伪标签，加入训练-selftraining：产生伪标签的3种方式） 2.一致性正则（核心在于最小化样本扰动前后预测的距离：区别在于数据扰动方式和距离度量）
同时考虑熵最小化和一致性正则的的整合工作：mixmatch，fixmatch
## 10 概率图模型
**概率模型**从概率角度描述/理解/建模机器学校任务，将任务转换为**计算变量的概率分布**
利用已知的变量推测未知变量的分布，**核心在于基于可观测的变量推测出未知变量的条件分布**

为了方便表达变量间关系-->概率图模型：用图来表达变量相关关系的概率模型（结点，边）
有向图：贝叶斯网，隐马/动态贝叶斯，LDA （都是生成式模型）
无向图：马尔可夫网（马尔可夫随机场MRF）

### HMM隐马尔可夫模型
结构图，观测变量，隐状态变量
**结构图，马尔可夫假设，联合概率公式**
状态空间Y，观测空间X和三组参数ABΠ-->确定HMM模型-->生成观测序列
### MRF马尔可夫随机场
基于**极大团**的**势函数（因子）** 团：任意两结点都有边连接的结点子集，极大团，
多个变量之间的连续分布可基于团分解为多个因子的乘积，每个因子只与一个团相关

马尔可夫随机场中的**条件独立性**： 
>>全局马尔可夫性：在给定分离集的条件下， 两个变量子集条件独立，记为xa ⊥ xb | xc  
>>全局导出：局部马尔可夫性，成对马尔可夫性  
马尔可夫随机场中的**势函数**： 

条件随机场CRF （判别式无向图模型）（可看作给定观测值的MRF）
目标：对多个变量给定相应观测值后的条件概率进行建模，即建模P(y|x)
每个结点都满足马尔可夫性，

链式条件随机场
>>结构图
>>两种关于标记变量的团
>>条件概率公式

### MRF/CRF
MRF：生成式模型，建模联合分布
CRF(给定观测变量的MRF，链式CRF)：判别式模型，建模条件概率分布
都使用团上的势函数定义/计算概率
其他一样：无向图模型，马尔可夫性，计算概率公式（基于极大团/两种团的势函数因子），势函数的偏好（刻画数据期望成立的经验特性）

（CRF标记变量y可以是结构性变量：nlp中词法分析-词性/线性序列、语法分析-语法树形结构）

### 概率图模型：学习和推断
学习->推断：**概率图模型的推断方法**
概率图模型中具体分布的参数：参数估计/学习（极大似然估计）-->（将其作为需要推断的参数）概率图模型推断
推断模型的目标：计算变量的边际分布或条件分布(贝叶斯公式=概率图模型获得联合分布/边际分布（指对其他变量求和求积分的结果）)->**关键在于高效计算边际分布**
模型推断方法：
1.精准推断（变量消去：最直接、最基础，信念传播：消息传播，修正，一个条件两个步骤） 
2.近似推断（1，随机性近似（采样-马尔可夫蒙特卡罗MCMC）2.确定性近似（变分推断））


**这部分公式推导的关键：1.联合概率，概率图模型公式 2.边际分布定义，无关变量求和求积分**
## 11 强化学习
从预测到决策
强化学习任务通常用马尔可夫决策过程MDP四元组表示<A,X,R,P>，图

机器学习最重要的任务是根据**已观察到的证据**（例如训练样本）对batch_tu!=None（例如类别标记）进行估计和推测



模型选择：经验误差，泛化误差（偏差-方差分解，由数据集质量、学习任务难度、模型学习能力）
SVM/统计学习模型：结构风险+经验风险
集成学习：误差分契分解

优缺点：决策树优缺点（可解释，过拟合），stacking，生成式方法/半监督svm/图半监督/基于分歧学习的优缺点，变量消去的优缺点
推导：knn理想误差的推导，adaboost，PCA两种等价，概率图（MRF的联合概率公式，全局马尔可夫性的验证）（模型精确推断：变量消去法-有向图\无向图不同的展开和使用元），强化学习（MRP马尔可夫回报过程）
算法描述：adaboost，半监督SVM，MCMC（关键在于构造平稳分布为q的马尔科夫链来产生样本），LDA文档生成过程/图模型/概率分布

死记硬背：特征选择方法-L1正则化项求解，半监督学习生成式方法推导高斯混合模型


一些假设：
朴素贝叶斯：假设属性相互独立
马尔可夫性质：系统的下一状态只与当前状态有关，与以往状态无关

贝叶斯网的条件独立性：转换为无向图，
马尔可夫随机场中的条件独立性
