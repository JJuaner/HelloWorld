## 2 模型评估和模型选择
经验误差（欠拟合过拟合，模型选择-经验误差）
泛化误差（误差=偏差方差分解，泛化能力）
评估方法、性能度量、比较检验
1-训练集测试集划分
2-回归、分类，ROC-AUC（样本预测的排序质量）
3-直接在测试集上比较性能的问题(123)，如果在测试集上观察到a比b好，则a统计意义上好于b的把握是多少
## 4 集成学习
用多个学习器提升性能解决问题
关键假设：基学习器的误差相互独立——>误差指数下降，趋于0
好而不同

集成学习方法两类：1.串行2.并行
boosting思路（）adaboost推导
bagging思路（自助采样法）

结合策略（好处3，方法3）

误差分歧分解-好而不同-多样性
## 5 聚类
聚类任务：无监督学习任务，将数据集划分成若干个通常不相交的簇
性能度量：簇内相似度高，簇间相似度低（聚类本身没有好坏之分，具体任务上效果有好坏之分）
关键：距离度量（距离特性，预定义/可学习，有序属性无序属性混合属性）

距离-KNN，聚类，度量学习
原型聚类（有簇中心的聚类方法）
LVQ：数据有类别标签，目标学习一组原型向量，刻画聚类簇结构
高斯混合聚类：用高斯分布刻画原型，均值簇中心，方差簇半径  假设样本由高斯混合分布生成（） 参数估计-极大似然估计EM算法

DBSCAN  邻域参数/关键概念(核心对象/密度直达可达互连)/算法描述（通过领域构建可达路径形成等价类联通分支）
## 6 降维
KNN：利用k个最近邻的信息来做预测（要点：k值，距离度量，如何利用:投票平均加权）
KNN特殊地位：1.懒惰算法，无需训练，但测试时需需要遍历一遍训练集 2.简单有效，泛化错误率不超过贝叶斯最优分类器错误率的两倍
懒惰算法、理论性质、现实不可行：KNN需要密采样，维度->样本数，现实情况样本少，样本稀疏
高维空间的维度灾难：计算开销大，最重要的是样本稀疏->降维，数据维度大，但任务相关的可能只是部分维度，高维空间的低维嵌入

线性降维（假设高维空间到低维空间是线性变化，统一形式，对低维空间性质的不同要求对W施加不同的约束，降维效果的）
MDS（思路：等距，D->B->Z 距离和内积的关系，谱特征分解）
PCA（样本中心化，两种等价推导，PCA求解（拉格朗日乘子法+特征值分解），d的选择）
非线性降维：核化线性方法，流形学习（ISOMAP，LLE)

度量学习：学习参数化的马氏距离
距离公式，度量矩阵M对称非负半正定PPt
不同的度量学习方法，不同的优化目标，不同的度量矩阵M
## 7 特征选择和稀疏表示
特征：定义，分类
特征选择：why
特征选择的一般方法：子集搜索，子集评估
特征选择方法：1.过滤式（relief，相关统计量）2.包裹式（lvw）3.嵌入式（L1正则化，w稀疏解非0分量，求解）

稀疏表示
将数据集看作一个矩阵，每行对应一个样本，每列对应一个特征
稀疏性：这个矩阵有很多0元素，但不是整行整列出现
稀疏表示的好处：1.高效存储 2.问题线性可分（svm文本数据）

学习一个字典，样本稠密表示->恰当的稀疏表示：字典学习
字典学习的目标是**字典矩阵B和样本的稀疏表示**，优化目标形式（1项最小化重构误差，2.尽量稀疏），求解（比L1更难解，变量交替优化（逐列更新策略））

压缩感知：限定等距性 优化形式（转换为共解的最小化L1范式问题）
矩阵补全：稀疏矩阵 优化形式（转换为矩阵核范数=奇异值之和->半正定规划）
## 8 半监督学习
主动学习、半监督学习、直推学习
未标记样本的假设（本质上都是：相似的样本具有相似的输出）
### 生成式方法
假设所有样本由一个潜在的生成式模型生成，EM算法极大似然估计求解
假设样本独立同分布，由一个高斯混合模型生成，每个类别对应一个高斯混合成分
这类方法的区别在于对生成式模型的假设，不同的假设形成不同的生成式方法
关键：模型假设必须正确，即假设的生成式模型必须与数据的真实分布吻合
适用于标签极少的情况
### 半监督SVM-TSVM
二分类问题
TSVM 思想（考虑为每个未标记样本分别为正例和负例的情况，然后在这些情况中找到间隔最大的划分超平面，这个超平面形成的划分作为最终的预测结果）和优化形式 
穷举过程->（未标记数量少，更高效）->TSVM局部搜索策略迭代的寻找近似解
算法描述，类别不平衡问题的改进，问题和改进

### 图半监督
给定数据集，将其映射为一个图（每个节点是一个样本，如果两个样本比较相似，则存在一条边，边的强度正比于样本的相似度）
图对应一个矩阵，基于矩阵运算来推导算法
图半监督学习的思想：颜色在图上的传播过程

二分类任务上单步传播
多分类任务上迭代式传播

亲和矩阵W 越小越相似
在图上学得f：V->R的实值函数，分类预测y=sign(f(x))
定义f的能量函数E(f)=公式推导/参数
能量函数越小越好，最小时得到最优结果



### 基于分歧的方法
多学习器，学习器间的分歧
多视图数据-协同训练（充分且条件独立，相容互补性--显著提升模型效果，更弱的情况也可以）
单视图数据-协同训练的变体（无需多视图，构建学习器之间的差异和分歧就可提升模型泛化性能）

### 拓展
半监督深度学习
动机
早期半监督深度学习方法（虽然用了有/无标签数据，单还是有监督的训练）：1.无监督预训练加微调（利用无标签数据让网络有好的初始化） 2.作为特征提取器
端到端的半监督深度学习两个基本方法：1.熵最小化（鼓励模型输出的预测置信度尽可能高，对未标记样本生成熵最小化的伪标签，加入训练-selftraining：产生伪标签的3种方式） 2.一致性正则（核心在于最小化样本扰动前后预测的距离：区别在于数据扰动方式和距离度量）
同时考虑熵最小化和一致性正则的的整合工作：mixmatch，fixmatch
## 10 概率图模型
**概率模型**从概率角度描述/理解/建模机器学校任务，将任务转换为**计算变量的概率分布**
利用已知的变量推测未知变量的分布，**核心在于基于可观测的变量推测出未知变量的条件分布**

为了方便表达变量间关系-->概率图模型：用图来表达变量相关关系的概率模型（结点，边）
有向图：贝叶斯网，隐马/动态贝叶斯，LDA （都是生成式模型）
无向图：马尔可夫网（马尔可夫随机场MRF）

### HMM隐马尔可夫模型
结构图，观测变量，隐状态变量
**结构图，马尔可夫假设，联合概率公式**
状态空间Y，观测空间X和三组参数ABΠ-->确定HMM模型-->生成观测序列
### MRF马尔可夫随机场
基于**极大团**的**势函数（因子）** 团：任意两结点都有边连接的结点子集，极大团，
多个变量之间的连续分布可基于团分解为多个因子的乘积，每个因子只与一个团相关

马尔可夫随机场中的**条件独立性**： 
>>全局马尔可夫性：在给定分离集的条件下， 两个变量子集条件独立，记为xa ⊥ xb | xc  
>>全局导出：局部马尔可夫性，成对马尔可夫性  
马尔可夫随机场中的**势函数**： 

条件随机场CRF （判别式无向图模型）（可看作给定观测值的MRF）
目标：对多个变量给定相应观测值后的条件概率进行建模，即建模P(y|x)
每个结点都满足马尔可夫性，

链式条件随机场
>>结构图
>>两种关于标记变量的团
>>条件概率公式

### MRF/CRF
MRF：生成式模型，建模联合分布
CRF(给定观测变量的MRF，链式CRF)：判别式模型，建模条件概率分布
都使用团上的势函数定义/计算概率
其他一样：无向图模型，马尔可夫性，计算概率公式（基于极大团/两种团的势函数因子），势函数的偏好（刻画数据期望成立的经验特性）

（CRF标记变量y可以是结构性变量：nlp中词法分析-词性/线性序列、语法分析-语法树形结构）

### 概率图模型：学习和推断
学习->推断：**概率图模型的推断方法**
概率图模型中具体分布的参数：参数估计/学习（极大似然估计）-->（将其作为需要推断的参数）概率图模型推断
推断模型的目标：计算变量的边际分布或条件分布(贝叶斯公式=概率图模型获得联合分布/边际分布（指对其他变量求和求积分的结果）)->**关键在于高效计算边际分布**
模型推断方法：
1.精准推断（变量消去：最直接、最基础，信念传播：消息传播，修正，一个条件两个步骤） 
2.近似推断（1，随机性近似（采样-马尔可夫蒙特卡罗MCMC）2.确定性近似（变分推断））

MCMC图模型中常用的采样方法
-MH算法
-吉布斯采样

变分推断：思想是将复杂的变量拆解成相互度量的变量，这些变量服从更简单的分布，用已知的简单分布逼近复杂分布-公式
如何拆解，假设什么分布，结合em算法参数估计，隐变量推断
### LDA话题模型


**这部分公式推导的关键：1.联合概率，概率图模型公式 2.边际分布定义，无关变量求和求积分**
## 11 强化学习
从预测到决策
强化学习任务通常用马尔可夫决策过程MDP四元组表示<A,X,R,P>描述（图）
强化学习目的：机器通过不断尝试学习一个累计奖赏最大的策略Π：
确定性策略X->A 概率性策略XA->R
策略优劣评价：累计奖赏（T步，折扣）

和监督学习的差别

机器学习最重要的任务是根据**已观察到的证据**（例如训练样本）对batch_tu!=None（例如类别标记）进行估计和推测



模型选择：经验误差，泛化误差（偏差-方差分解，由数据集质量、学习任务难度、模型学习能力）
SVM/统计学习模型：结构风险+经验风险
集成学习：误差分契分解

优缺点：决策树优缺点（可解释，过拟合），stacking，生成式方法/半监督svm/图半监督/基于分歧学习的优缺点，变量消去的优缺点
推导：knn理想误差的推导，adaboost，PCA两种等价，7.字典学习，8.概率图（MRF的联合概率公式，全局马尔可夫性的验证）（模型精确推断：变量消去法-有向图\无向图不同的展开和使用元），强化学习（MRP马尔可夫回报过程）

算法描述：
adaboost
LVQ、DBSCAN、AGNES
半监督SVM，
概率图：MCMC（关键在于构造平稳分布为q的马尔科夫链来产生样本），LDA文档生成过程/图模型/概率分布

死记硬背：
adaboost
特征选择方法-L1正则化项求解
半监督学习生成式方法推导高斯混合模型
图半监督学习
变分推断


一些假设：
朴素贝叶斯：假设属性相互独立
马尔可夫性质：系统的下一状态只与当前状态有关，与以往状态无关

贝叶斯网的条件独立性：转换为无向图，
马尔可夫随机场中的条件独立性

SVM/adaboost/图半监督 二分类

高斯混合模型：高斯混合聚类，半监督生成式方法
EM算法：高斯混合分布

近邻：基于密度的聚类DBSCAN，ISOmap，LLE，图，马尔可夫性

注意：LVQ的+—方向，
PCA优化目标tr，
马尔可夫链平稳条件
L1求解多个1/L
CRF条件概率公式多个jk
LDA的几个公式
累计奖赏起始，MRP公式中多个a


概率的语义理解
求和求乘的三种理解

求解方法：求导=0，似然函数EM，特征分解，概率图模型推断变分法，L1，pd
